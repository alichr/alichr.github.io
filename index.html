<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="Ali Cheraghian - Senior AI/ML Scientist at Macquarie University specializing in Vision-Language Models, LLMs, and Computer Vision">
    <meta name="keywords" content="Ali Cheraghian, AI, Machine Learning, Computer Vision, Deep Learning, Research Scientist, Macquarie University, CVPR, ICCV, ECCV">
    <meta name="author" content="Ali Cheraghian">
    <meta name="theme-color" content="#002b5e" id="theme-color-meta">

    <!-- Open Graph / Social Media -->
    <meta property="og:type" content="website">
    <meta property="og:title" content="Ali Cheraghian - Senior AI/ML Scientist">
    <meta property="og:description" content="Senior AI/ML Scientist at Macquarie University. Research in Vision-Language Models, LLMs, Knowledge Distillation, and 3D Point Cloud Understanding.">
    <meta property="og:image" content="https://alichr.github.io/images/profile.webp">
    <meta property="og:url" content="https://alichr.github.io">
    <meta property="og:site_name" content="Ali Cheraghian">

    <!-- Twitter Card -->
    <meta name="twitter:card" content="summary_large_image">
    <meta name="twitter:title" content="Ali Cheraghian - Senior AI/ML Scientist">
    <meta name="twitter:description" content="Senior AI/ML Scientist at Macquarie University. Research in Vision-Language Models, LLMs, and Computer Vision.">
    <meta name="twitter:image" content="https://alichr.github.io/images/profile.webp">

    <!-- Favicon -->
    <link rel="icon" type="image/svg+xml" href="data:image/svg+xml,<svg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 100 100'><text y='.9em' font-size='90'>ðŸŽ“</text></svg>">
    <link rel="apple-touch-icon" href="./images/profile.webp">

    <title>Ali Cheraghian - Senior AI/ML Scientist</title>
    <link rel="stylesheet" href="styles.css">
    <link href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;500;600;700&display=swap" rel="stylesheet">
    <script src="https://cdn.jsdelivr.net/npm/chart.js"></script>
    <script>
        // Prevent flash of wrong theme
        (function() {
            const savedTheme = localStorage.getItem('theme');
            const prefersDark = window.matchMedia('(prefers-color-scheme: dark)').matches;

            if (savedTheme === 'dark' || (!savedTheme && prefersDark)) {
                document.documentElement.classList.add('dark-mode');
            }
        })();
    </script>
</head>

<body>
    <!-- Skip Link for Accessibility -->
    <a href="#main-content" class="skip-link">Skip to main content</a>

    <header role="banner">
        <div class="header-content">
            <!-- Dark Mode Toggle -->
            <button class="theme-toggle" id="theme-toggle" aria-label="Toggle dark mode" type="button">
                <span class="sun-icon" aria-hidden="true">&#9728;</span>
                <span class="moon-icon" aria-hidden="true">&#9790;</span>
            </button>

            <img src="./images/profile.webp" alt="Ali Cheraghian - Profile Photo" class="profile-photo" loading="lazy">
            <h1>Ali Cheraghian</h1>
            <p class="title">Senior AI/ML Scientist | Macquarie University</p>

            <nav class="contact-links" role="navigation" aria-label="Contact and social links">
                <a href="mailto:ali.cheraghian@mq.edu.au" aria-label="Send email to Ali Cheraghian" class="social-link">
                    <svg class="social-icon" viewBox="0 0 24 24" fill="currentColor" aria-hidden="true"><path d="M20 4H4c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h16c1.1 0 2-.9 2-2V6c0-1.1-.9-2-2-2zm0 4l-8 5-8-5V6l8 5 8-5v2z"/></svg>
                    <span>Email</span>
                </a>
                <a href="https://www.linkedin.com/in/ali-cheraghian/" target="_blank" rel="noopener noreferrer" aria-label="View LinkedIn profile (opens in new tab)" class="social-link">
                    <svg class="social-icon" viewBox="0 0 24 24" fill="currentColor" aria-hidden="true"><path d="M19 3a2 2 0 0 1 2 2v14a2 2 0 0 1-2 2H5a2 2 0 0 1-2-2V5a2 2 0 0 1 2-2h14m-.5 15.5v-5.3a3.26 3.26 0 0 0-3.26-3.26c-.85 0-1.84.52-2.32 1.3v-1.11h-2.79v8.37h2.79v-4.93c0-.77.62-1.4 1.39-1.4a1.4 1.4 0 0 1 1.4 1.4v4.93h2.79M6.88 8.56a1.68 1.68 0 0 0 1.68-1.68c0-.93-.75-1.69-1.68-1.69a1.69 1.69 0 0 0-1.69 1.69c0 .93.76 1.68 1.69 1.68m1.39 9.94v-8.37H5.5v8.37h2.77z"/></svg>
                    <span>LinkedIn</span>
                </a>
                <a href="https://github.com/alichr" target="_blank" rel="noopener noreferrer" aria-label="View GitHub profile (opens in new tab)" class="social-link">
                    <svg class="social-icon" viewBox="0 0 24 24" fill="currentColor" aria-hidden="true"><path d="M12 2A10 10 0 0 0 2 12c0 4.42 2.87 8.17 6.84 9.5.5.08.66-.23.66-.5v-1.69c-2.77.6-3.36-1.34-3.36-1.34-.46-1.16-1.11-1.47-1.11-1.47-.91-.62.07-.6.07-.6 1 .07 1.53 1.03 1.53 1.03.87 1.52 2.34 1.07 2.91.83.09-.65.35-1.09.63-1.34-2.22-.25-4.55-1.11-4.55-4.92 0-1.11.38-2 1.03-2.71-.1-.25-.45-1.29.1-2.64 0 0 .84-.27 2.75 1.02.79-.22 1.65-.33 2.5-.33.85 0 1.71.11 2.5.33 1.91-1.29 2.75-1.02 2.75-1.02.55 1.35.2 2.39.1 2.64.65.71 1.03 1.6 1.03 2.71 0 3.82-2.34 4.66-4.57 4.91.36.31.69.92.69 1.85V21c0 .27.16.59.67.5C19.14 20.16 22 16.42 22 12A10 10 0 0 0 12 2z"/></svg>
                    <span>GitHub</span>
                </a>
                <a href="https://scholar.google.com/citations?user=QT0EXIkAAAAJ" target="_blank" rel="noopener noreferrer" aria-label="View Google Scholar profile (opens in new tab)" class="social-link">
                    <svg class="social-icon" viewBox="0 0 24 24" fill="currentColor" aria-hidden="true"><path d="M5.242 13.769L0 9.5 12 0l12 9.5-5.242 4.269C17.548 11.249 14.978 9.5 12 9.5c-2.977 0-5.548 1.748-6.758 4.269zM12 10a7 7 0 1 0 0 14 7 7 0 0 0 0-14z"/></svg>
                    <span>Scholar</span>
                </a>
            </nav>

            <div class="bio-text">
                <p>I am a <strong>Senior AI/ML Scientist</strong> at Macquarie University's Silicon Platforms Lab, an <strong>Adjunct Faculty Member</strong> at the Australian National University (ANU), and former <strong>Computer Vision Scientist</strong> at CSIRO Data61.</p>

                <p class="research-interests"><strong>Research Interests:</strong> Vision-Language Models (VLMs), Large Language Models (LLMs), Generative AI, Knowledge Distillation, Few-Shot Learning, Incremental Learning, 3D Point Cloud Understanding, and Test-Time Adaptation.</p>

                <p>My research bridges cutting-edge AI methods with real-world applications, with publications in top-tier venues including <strong>CVPR</strong>, <strong>ICCV</strong>, <strong>ECCV</strong>, <strong>ICLR</strong>, <strong>AAAI</strong>, and <strong>IJCV</strong>. Currently, I focus on applying advanced AI to integrated circuit design and semiconductor innovation.</p>
            </div>
        </div>
    </header>

    <main id="main-content" role="main">
        <section class="news-section" aria-labelledby="news-heading">
            <h2 id="news-heading">News</h2>
            <ul>
                <li>Our paper "DTO-KD: Dynamic Trade-off Optimization for Effective Knowledge Distillation" has been accepted to <span class="venue">ICLR'26</span></li>
                <li>Our paper "Adapt-As-You-Walk Through the Clouds: Training-Free Online Test-Time Adaptation of 3D Vision-Language Foundation Models" has been accepted to <span class="venue">AAAI'26</span></li>
                <li>Our paper "SDMD: Subspace-Driven Model Distillation in Indefinite Inner Product Spaces" has been accepted to <span class="venue">WACV'26</span></li>
                <li>I have started a new position as Senior AI/ML Scientist at Macquarie University's Silicon Platforms Lab starting from <span class="venue">August 2025</span></li>
                <li>Our paper "LumiNet: Perception-Driven Knowledge Distillation via Statistical Logit Calibration" has been accepted to <span class="venue">TMLR'25</span></li>
                <li>Our papers "ETTA: Efficient Test-Time Adaptation for Vision-Language Models" and "Task Progressive Curriculum Learning" have been accepted to <span class="venue">BMVC'25</span></li>
                <li>Our paper "Test-Time Adaptation of 3D Point Clouds via Denoising Diffusion Models" has been accepted to <span class="venue">WACV'25</span></li>
                <li>Our paper "Canonical shape projection is all you need for 3d few-shot class incremental learning" has been accepted to <span class="venue">ECCV'24</span></li>
                <li>Our paper "Backpropagation-free Network for 3D Test-time Adaptation" has been accepted to <span class="venue">CVPR'24</span></li>
                <li>Our paper "Continual test-time domain adaptation via dynamic sample selection" has been accepted to <span class="venue">WACV'24</span></li>
                <li>Our paper "Foundation Model-Powered 3D Few-Shot Class Incremental Learning via Training-free Adaptor" has been accepted to <span class="venue">ACCV'24</span></li>
                <li>Our paper "3D Point Cloud Network Pruning: When Some Weights Do not Matter" has been accepted to <span class="venue">BMVC'24</span></li>
                <li>Our paper "3D scene generation for zero-shot learning using ChatGPT guided language prompts" has been accepted to <span class="venue">CVIU'24</span></li>
                <li>Our papers "Efficient Atmospheric Correction" and "Enhancing Glaucoma Diagnosis" have been accepted to <span class="venue">DICTA'24</span></li>
            </ul>
        </section>

        <section class="chatbot-section" aria-labelledby="chatbot-heading">
            <h2 id="chatbot-heading">Ask Me Anything</h2>
            <div class="chatbot-description">
                <p>Have questions about my research, career, or want to know more about my work? Chat with my AI assistant that knows about my background, publications, and expertise. Feel free to ask about my research areas, publications, or any other questions you might have!</p>
            </div>
            <div class="chatbot-container">
                <div class="chatbot-placeholder">
                    <div class="chatbot-icon" aria-hidden="true">&#129302;</div>
                    <h3>Chat with Ali's AI Assistant</h3>
                    <p>Engage with my intelligent AI assistant that has comprehensive knowledge about my research journey, publications, and expertise in AI/ML, computer vision, and generative technologies.</p>

                    <div class="sample-questions" role="list" aria-label="Sample questions you can ask">
                        <span class="sample-question" role="listitem" tabindex="0">What are your main research areas?</span>
                        <span class="sample-question" role="listitem" tabindex="0">Tell me about your publications</span>
                        <span class="sample-question" role="listitem" tabindex="0">What's your career journey?</span>
                        <span class="sample-question" role="listitem" tabindex="0">How can I collaborate?</span>
                    </div>

                    <a href="https://huggingface.co/spaces/alichr/career_conversation" target="_blank" rel="noopener noreferrer" class="chatbot-launch-button" aria-label="Start conversation with AI assistant (opens in new tab)">
                        &#128640; Start Conversation
                    </a>

                    <div class="ai-features" role="list" aria-label="AI assistant features">
                        <div class="ai-feature" role="listitem">
                            <span class="ai-feature-icon" aria-hidden="true">&#129504;</span>
                            <div class="ai-feature-text">Research Expert</div>
                        </div>
                        <div class="ai-feature" role="listitem">
                            <span class="ai-feature-icon" aria-hidden="true">&#128218;</span>
                            <div class="ai-feature-text">Publication Knowledge</div>
                        </div>
                        <div class="ai-feature" role="listitem">
                            <span class="ai-feature-icon" aria-hidden="true">&#127919;</span>
                            <div class="ai-feature-text">Collaboration Ready</div>
                        </div>
                    </div>
                </div>
            </div>
        </section>

        <section class="publications-section" aria-labelledby="publications-heading">
            <h2 id="publications-heading">Publications</h2>
            <div class="stats-container">
                <div class="citation-graph" aria-label="Citations over time chart">
                    <h3>Citations Over Time</h3>
                    <canvas id="citationChart" aria-label="Bar chart showing citation counts from 2017 to 2026"></canvas>
                </div>
                <div class="citation-stats" aria-labelledby="citation-metrics-heading">
                    <h3 id="citation-metrics-heading">Citation Metrics</h3>
                    <div class="stat-item">
                        <span class="stat-label">Total Citations</span>
                        <span class="stat-value">929</span>
                    </div>
                    <div class="stat-item">
                        <span class="stat-label">h-index</span>
                        <span class="stat-value">12</span>
                    </div>
                    <div class="stat-item">
                        <span class="stat-label">i10-index</span>
                        <span class="stat-value">13</span>
                    </div>
                </div>
            </div>

            <div class="publications-list">
                <h3 class="sr-only">Publication List by Year</h3>

                <h4>2026</h4>
                <ul>
                    <li>
                        <img src="./images/papers/Adapt-As-You-Walk Through the Clouds: Training-Free Online Test-Time Adaptation of 3D Vision-Language Foundation Models.webp" alt="" class="publication-thumbnail" loading="lazy">
                        <article class="publication-content">
                            <a href="#" target="_blank" rel="noopener noreferrer"><strong>Adapt-As-You-Walk Through the Clouds: Training-Free Online Test-Time Adaptation of 3D Vision-Language Foundation Models</strong></a><br>
                            Mehran Tamjidi, Hamidreza Dastmalchi, Mohammadreza Alimoradijazi, Ali Cheraghian, Aijun An, Morteza Saberi<br>
                            AAAI 2026
                        </article>
                    </li>
                    <li>
                        <img src="./images/papers/SDMD: Subspace-Driven Model Distillation in Indefinite Inner Product Spaces.webp" alt="" class="publication-thumbnail" loading="lazy">
                        <article class="publication-content">
                            <a href="#" target="_blank" rel="noopener noreferrer"><strong>SDMD: Subspace-Driven Model Distillation in Indefinite Inner Product Spaces</strong></a><br>
                            Zeeshan Hayder, Ali Cheraghian, Lars Petersson, Mehrtash Harandi<br>
                            WACV 2026
                        </article>
                    </li>
                    <li>
                        <img src="./images/papers/DTO-KD_Dynamic_Trade-off_Optimization_for_Effective_Knowledge_Distillation.webp" alt="" class="publication-thumbnail" loading="lazy">
                        <article class="publication-content">
                            <a href="#" target="_blank" rel="noopener noreferrer"><strong>DTO-KD: Dynamic Trade-off Optimization for Effective Knowledge Distillation</strong></a><br>
                            Zeeshan Hayder, Ali Cheraghian, Lars Petersson, Mehrtash Harandi, Richard Hartley<br>
                            ICLR 2026
                        </article>
                    </li>
                </ul>

                <h4>2025</h4>
                <ul>
                    <li>
                        <img src="./images/papers/LumiNet: Perception-Driven Knowledge Distillation.webp" alt="" class="publication-thumbnail" loading="lazy">
                        <article class="publication-content">
                            <a href="#" target="_blank" rel="noopener noreferrer"><strong>LumiNet: Perception-Driven Knowledge Distillation via Statistical Logit Calibration</strong></a><br>
                            MI Hossain, MM Lutfe Elahi, S Ramasinghe, A Cheraghian, F Rahman, N Mohammed, S Rahman<br>
                            TMLR 2025
                        </article>
                    </li>
                    <li>
                        <img src="./images/papers/ETTA_Efficient Test-Time Adaptation for Vision-Language Models.webp" alt="" class="publication-thumbnail" loading="lazy">
                        <article class="publication-content">
                            <a href="#" target="_blank" rel="noopener noreferrer"><strong>ETTA: Efficient Test-Time Adaptation for Vision-Language Models through Dynamic Embedding Updates</strong></a><br>
                            H Dastmalchi, A An, A Cheraghian<br>
                            BMVC 2025
                        </article>
                    </li>
                    <li>
                        <img src="./images/papers/Task Progressive Curriculum Learning.webp" alt="" class="publication-thumbnail" loading="lazy">
                        <article class="publication-content">
                            <a href="#" target="_blank" rel="noopener noreferrer"><strong>Task Progressive Curriculum Learning for Robust Visual Question Answering</strong></a><br>
                            A Akl, A Khamis, Z Wang, A Cheraghian, S Khalifa, K Wang<br>
                            BMVC 2025
                        </article>
                    </li>
                    <li>
                        <img src="./images/papers/Test_Time_Adaptation_3D_Point_Clouds.webp" alt="" class="publication-thumbnail" loading="lazy">
                        <article class="publication-content">
                            <a href="https://arxiv.org/pdf/2411.14495" target="_blank" rel="noopener noreferrer"><strong>Test-Time Adaptation of 3D Point Clouds via Denoising Diffusion Models</strong></a><br>
                            H Dastmalchi, A An, A Cheraghian, S Rahman, S Ramasinghe<br>
                            WACV 2025
                        </article>
                    </li>
                </ul>

                <h4>2024</h4>
                <ul>
                    <li>
                        <img src="./images/papers/Backpropagation-free_Network_for_3D_Test-time_Adaptation.webp" alt="" class="publication-thumbnail" loading="lazy">
                        <article class="publication-content">
                            <a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Wang_Backpropagation-free_Network_for_3D_Test-time_Adaptation_CVPR_2024_paper.pdf" target="_blank" rel="noopener noreferrer"><strong>Backpropagation-free Network for 3D Test-time Adaptation</strong></a><br>
                            Y Wang, A Cheraghian, Z Hayder, J Hong, S Ramasinghe, S Rahman<br>
                            CVPR 2024
                        </article>
                    </li>
                    <li>
                        <img src="./images/papers/Continual_test-time_domain_adaptation_via_dynamic_sample_selection.webp" alt="" class="publication-thumbnail" loading="lazy">
                        <article class="publication-content">
                            <a href="https://openaccess.thecvf.com/content/WACV2024/papers/Wang_Continual_Test-Time_Domain_Adaptation_via_Dynamic_Sample_Selection_WACV_2024_paper.pdf" target="_blank" rel="noopener noreferrer"><strong>Continual test-time domain adaptation via dynamic sample selection</strong></a><br>
                            Y Wang, J Hong, A Cheraghian, S Rahman, D Ahmedt-Aristizabal<br>
                            WACV 2024
                        </article>
                    </li>
                    <li>
                        <img src="./images/papers/Canonical_shape_projection_is_all_you_need_for_3d_few-shot_class_incremental_learning.webp" alt="" class="publication-thumbnail" loading="lazy">
                        <article class="publication-content">
                            <a href="https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/05717.pdf" target="_blank" rel="noopener noreferrer"><strong>Canonical shape projection is all you need for 3d few-shot class incremental learning</strong></a><br>
                            A Cheraghian, Z Hayder, S Ramasinghe, S Rahman, J Jafaryahya<br>
                            ECCV 2024
                        </article>
                    </li>
                    <li>
                        <img src="./images/papers/Foundation_Model-Powered_3D_Few-Shot_Class_Incremental_Learning_via_Training-free_Adaptor.webp" alt="" class="publication-thumbnail" loading="lazy">
                        <article class="publication-content">
                            <a href="https://openaccess.thecvf.com/content/ACCV2024/papers/Ahmadi_Foundation_Model-Powered_3D_Few-Shot_Class_Incremental_Learning_via_Training-free_Adaptor_ACCV_2024_paper.pdf" target="_blank" rel="noopener noreferrer"><strong>Foundation Model-Powered 3D Few-Shot Class Incremental Learning via Training-free Adaptor</strong></a><br>
                            S Ahmadi, A Cheraghian, M Saberi, MT Abir, H Dastmalchi, F Hussain<br>
                            ACCV 2024
                        </article>
                    </li>
                    <li>
                        <img src="./images/papers/3D_Point_Cloud_Network_Pruning_When_Some_Weights_Do_not_Matter.webp" alt="" class="publication-thumbnail" loading="lazy">
                        <article class="publication-content">
                            <a href="https://bmva-archive.org.uk/bmvc/2024/papers/Paper_637/paper.pdf" target="_blank" rel="noopener noreferrer"><strong>3D Point Cloud Network Pruning: When Some Weights Do not Matter</strong></a><br>
                            A Biswas, MI Hossain, MM Elahi, A Cheraghian, F Rahman<br>
                            BMVC 2024
                        </article>
                    </li>
                    <li>
                        <img src="./images/papers/3D_scene_generation_for_zero-shot_learning_using_ChatGPT_guided_language_prompts.webp" alt="" class="publication-thumbnail" loading="lazy">
                        <article class="publication-content">
                            <a href="https://www.sciencedirect.com/science/article/pii/S1077314224002923" target="_blank" rel="noopener noreferrer"><strong>3D scene generation for zero-shot learning using ChatGPT guided language prompts</strong></a><br>
                            S Ahmadi, A Cheraghian, TF Chowdhury, M Saberi, S Rahman<br>
                            CVIU 2024
                        </article>
                    </li>
                    <li>
                        <img src="./images/papers/Efficient_Atmospheric_Correction.webp" alt="" class="publication-thumbnail" loading="lazy">
                        <article class="publication-content">
                            <a href="https://ieeexplore.ieee.org/document/10869604" target="_blank" rel="noopener noreferrer"><strong>Efficient Atmospheric Correction for Onboard Processing Using Knowledge Distillation and Model Compression</strong></a><br>
                            M Zhang, A Cheraghian, Y Qin, D Benn, T Rollan, N Habili<br>
                            DICTA 2024
                        </article>
                    </li>
                    <li>
                        <img src="./images/papers/Enhancing_Glaucoma_Diagnosis.webp" alt="" class="publication-thumbnail" loading="lazy">
                        <article class="publication-content">
                            <a href="https://ieeexplore.ieee.org/document/10869527" target="_blank" rel="noopener noreferrer"><strong>Enhancing Glaucoma Diagnosis through Vision-Language Models and Large Language Model Descriptions</strong></a><br>
                            HY Bae, M Saberi, S Shariflou, M Kalloniatis, J Phu, A Agar, A Cheraghian<br>
                            DICTA 2024
                        </article>
                    </li>
                </ul>

                <h4>2023</h4>
                <ul>
                    <li>
                        <img src="./images/papers/ChatGPT-guided_Semantics_for_Zero-shot_Learning.webp" alt="" class="publication-thumbnail" loading="lazy">
                        <article class="publication-content">
                            <a href="https://arxiv.org/pdf/2310.11657" target="_blank" rel="noopener noreferrer"><strong>ChatGPT-guided Semantics for Zero-shot Learning</strong></a><br>
                            FH Shubho, TF Chowdhury, A Cheraghian, M Saberi, N Mohammed<br>
                            DICTA 2023
                        </article>
                    </li>
                </ul>

                <h4>2022</h4>
                <ul>
                    <li>
                        <img src="./images/papers/Zero-shot_learning_on_3d_point_cloud_objects_and_beyond.webp" alt="" class="publication-thumbnail" loading="lazy">
                        <article class="publication-content">
                            <a href="https://arxiv.org/pdf/2104.04980" target="_blank" rel="noopener noreferrer"><strong>Zero-shot learning on 3d point cloud objects and beyond</strong></a><br>
                            A Cheraghian, S Rahman, TF Chowdhury, D Campbell, L Petersson<br>
                            IJCV 2022
                        </article>
                    </li>
                    <li>
                        <img src="./images/papers/Few-shot_class-incremental_learning_for_3d_point_cloud_objects.webp" alt="" class="publication-thumbnail" loading="lazy">
                        <article class="publication-content">
                            <a href="https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136800194.pdf" target="_blank" rel="noopener noreferrer"><strong>Few-shot class-incremental learning for 3d point cloud objects</strong></a><br>
                            T Chowdhury, A Cheraghian, S Ramasinghe, S Ahmadi, M Saberi<br>
                            ECCV 2022
                        </article>
                    </li>
                    <li>
                        <img src="./images/papers/Prompt-guided_scene_generation_for_3d_zero-shot_learning.webp" alt="" class="publication-thumbnail" loading="lazy">
                        <article class="publication-content">
                            <a href="https://arxiv.org/pdf/2209.14690" target="_blank" rel="noopener noreferrer"><strong>Prompt-guided scene generation for 3d zero-shot learning</strong></a><br>
                            M Nasiri, A Cheraghian, TF Chowdhury, S Ahmadi, M Saberi, S Rahman<br>
                            DICTA 2022
                        </article>
                    </li>
                </ul>

                <h4>2021</h4>
                <ul>
                    <li>
                        <img src="./images/papers/Semantic-aware_knowledge_distillation_for_few-shot_class-incremental_learning.webp" alt="" class="publication-thumbnail" loading="lazy">
                        <article class="publication-content">
                            <a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Cheraghian_Semantic-Aware_Knowledge_Distillation_for_Few-Shot_Class-Incremental_Learning_CVPR_2021_paper.pdf" target="_blank" rel="noopener noreferrer"><strong>Semantic-aware knowledge distillation for few-shot class-incremental learning</strong></a><br>
                            A Cheraghian, S Rahman, P Fang, SK Roy, L Petersson, M Harandi<br>
                            CVPR 2021
                        </article>
                    </li>
                    <li>
                        <img src="./images/papers/Synthesized_feature_based_few-shot_class-incremental_learning_on_a_mixture_of_subspaces.webp" alt="" class="publication-thumbnail" loading="lazy">
                        <article class="publication-content">
                            <a href="https://openaccess.thecvf.com/content/ICCV2021/papers/Cheraghian_Synthesized_Feature_Based_Few-Shot_Class-Incremental_Learning_on_a_Mixture_of_ICCV_2021_paper.pdf" target="_blank" rel="noopener noreferrer"><strong>Synthesized feature based few-shot class-incremental learning on a mixture of subspaces</strong></a><br>
                            A Cheraghian, S Rahman, S Ramasinghe, P Fang, C Simon, L Petersson<br>
                            ICCV 2021
                        </article>
                    </li>
                    <li>
                        <img src="./images/papers/Learning_without_forgetting_for_3d_point_cloud_objects.webp" alt="" class="publication-thumbnail" loading="lazy">
                        <article class="publication-content">
                            <a href="https://arxiv.org/pdf/2106.14275" target="_blank" rel="noopener noreferrer"><strong>Learning without forgetting for 3d point cloud objects</strong></a><br>
                            T Chowdhury, M Jalisha, A Cheraghian, S Rahman<br>
                            IWANN 2021
                        </article>
                    </li>
                </ul>

                <h4>2020</h4>
                <ul>
                    <li>
                        <img src="./images/papers/Transductive_zero-shot_learning_for_3d_point_cloud_classification.webp" alt="" class="publication-thumbnail" loading="lazy">
                        <article class="publication-content">
                            <a href="https://openaccess.thecvf.com/content_WACV_2020/papers/Cheraghian_Transductive_Zero-Shot_Learning_for_3D_Point_Cloud_Classification_WACV_2020_paper.pdf" target="_blank" rel="noopener noreferrer"><strong>Transductive zero-shot learning for 3d point cloud classification</strong></a><br>
                            A Cheraghian, S Rahman, D Campbell, L Petersson<br>
                            WACV 2020
                        </article>
                    </li>
                </ul>

                <h4>Earlier Publications</h4>
                <ul>
                    <li>
                        <img src="./images/papers/Zero-shot_learning_of_3d_point_cloud_objects.webp" alt="" class="publication-thumbnail" loading="lazy">
                        <article class="publication-content">
                            <a href="https://arxiv.org/pdf/1902.10272" target="_blank" rel="noopener noreferrer"><strong>Zero-shot learning of 3d point cloud objects</strong></a><br>
                            A Cheraghian, S Rahman, L Petersson<br>
                            MVA 2019
                        </article>
                    </li>
                    <li>
                        <img src="./images/papers/3dcapsule_Extending_the_capsule_architecture_to_classify_3d_point_clouds.webp" alt="" class="publication-thumbnail" loading="lazy">
                        <article class="publication-content">
                            <a href="https://arxiv.org/pdf/1811.02191" target="_blank" rel="noopener noreferrer"><strong>3dcapsule: Extending the capsule architecture to classify 3d point clouds</strong></a><br>
                            A Cheraghian, L Petersson<br>
                            WACV 2019
                        </article>
                    </li>
                    <li>
                        <img src="./images/papers/Surface_geodesic_pattern_for_3D_deformable_texture_matching.webp" alt="" class="publication-thumbnail" loading="lazy">
                        <article class="publication-content">
                            <a href="https://www.sciencedirect.com/science/article/abs/pii/S0031320316302321" target="_blank" rel="noopener noreferrer"><strong>Surface geodesic pattern for 3D deformable texture matching</strong></a><br>
                            F Hajati, A Cheraghian, S Gheisari, Y Gao, AS Mian<br>
                            Pattern Recognition 2017
                        </article>
                    </li>
                </ul>
            </div>
        </section>
    </main>

    <footer role="contentinfo">
        <div class="footer-content">
            <p class="footer-copyright">&copy; <span id="current-year">2026</span> Ali Cheraghian. All rights reserved.</p>
            <div class="visitor-counter" aria-label="Visitor counter">
                <span class="visitor-counter-label">Visitors:</span>
                <a href="https://info.flagcounter.com/8m5j" target="_blank" rel="noopener noreferrer" aria-label="View visitor statistics">
                    <img src="https://s11.flagcounter.com/mini/8m5j/bg_FFFFFF/txt_000000/border_CCCCCC/flags_0/" alt="Visitor Counter">
                </a>
            </div>
        </div>
    </footer>

    <script>
        // Dark mode toggle functionality
        (function() {
            const themeToggle = document.getElementById('theme-toggle');
            const themeColorMeta = document.getElementById('theme-color-meta');

            function updateThemeColor() {
                const isDark = document.documentElement.classList.contains('dark-mode');
                themeColorMeta.setAttribute('content', isDark ? '#0d1117' : '#002b5e');
            }

            themeToggle.addEventListener('click', function() {
                document.documentElement.classList.toggle('dark-mode');
                const isDark = document.documentElement.classList.contains('dark-mode');
                localStorage.setItem('theme', isDark ? 'dark' : 'light');
                updateThemeColor();
            });

            // Update theme color on load
            updateThemeColor();

            // Listen for system preference changes
            window.matchMedia('(prefers-color-scheme: dark)').addEventListener('change', function(e) {
                if (!localStorage.getItem('theme')) {
                    if (e.matches) {
                        document.documentElement.classList.add('dark-mode');
                    } else {
                        document.documentElement.classList.remove('dark-mode');
                    }
                    updateThemeColor();
                }
            });
        })();

        // Update copyright year
        document.getElementById('current-year').textContent = new Date().getFullYear();

        // Citation chart
        const ctx = document.getElementById('citationChart').getContext('2d');
        const isDarkMode = document.documentElement.classList.contains('dark-mode');

        new Chart(ctx, {
            type: 'bar',
            data: {
                labels: ['2017', '2018', '2019', '2020', '2021', '2022', '2023', '2024', '2025', '2026'],
                datasets: [{
                    label: 'Citations',
                    data: [3, 5, 8, 25, 44, 113, 207, 228, 270, 15], // citation data per year
                    backgroundColor: [
                        'rgba(255, 99, 132, 0.7)',
                        'rgba(54, 162, 235, 0.7)',
                        'rgba(255, 206, 86, 0.7)',
                        'rgba(75, 192, 192, 0.7)',
                        'rgba(153, 102, 255, 0.7)',
                        'rgba(255, 159, 64, 0.7)',
                        'rgba(46, 204, 113, 0.7)',
                        'rgba(142, 68, 173, 0.7)',
                        'rgba(52, 152, 219, 0.7)',
                        'rgba(230, 126, 34, 0.7)'
                    ],
                    borderColor: [
                        'rgb(255, 99, 132)',
                        'rgb(54, 162, 235)',
                        'rgb(255, 206, 86)',
                        'rgb(75, 192, 192)',
                        'rgb(153, 102, 255)',
                        'rgb(255, 159, 64)',
                        'rgb(46, 204, 113)',
                        'rgb(142, 68, 173)',
                        'rgb(52, 152, 219)',
                        'rgb(230, 126, 34)'
                    ],
                    borderWidth: 1,
                    borderRadius: 5,
                    hoverBackgroundColor: [
                        'rgba(255, 99, 132, 0.9)',
                        'rgba(54, 162, 235, 0.9)',
                        'rgba(255, 206, 86, 0.9)',
                        'rgba(75, 192, 192, 0.9)',
                        'rgba(153, 102, 255, 0.9)',
                        'rgba(255, 159, 64, 0.9)',
                        'rgba(46, 204, 113, 0.9)',
                        'rgba(142, 68, 173, 0.9)',
                        'rgba(52, 152, 219, 0.9)',
                        'rgba(230, 126, 34, 0.9)'
                    ]
                }]
            },
            options: {
                responsive: true,
                maintainAspectRatio: false,
                plugins: {
                    legend: {
                        display: false
                    }
                },
                scales: {
                    y: {
                        beginAtZero: true,
                        grid: {
                            color: isDarkMode ? 'rgba(255, 255, 255, 0.1)' : 'rgba(0, 0, 0, 0.05)'
                        },
                        ticks: {
                            font: {
                                family: 'Montserrat'
                            },
                            color: isDarkMode ? '#c9d1d9' : '#333'
                        }
                    },
                    x: {
                        grid: {
                            display: false
                        },
                        ticks: {
                            font: {
                                family: 'Montserrat'
                            },
                            color: isDarkMode ? '#c9d1d9' : '#333'
                        }
                    }
                },
                animation: {
                    duration: 2000,
                    easing: 'easeInOutQuart'
                },
                hover: {
                    mode: 'nearest',
                    intersect: true
                }
            }
        });

        // Interactive sample questions for chatbot
        document.addEventListener('DOMContentLoaded', function() {
            const sampleQuestions = document.querySelectorAll('.sample-question');
            const chatbotButton = document.querySelector('.chatbot-launch-button');

            sampleQuestions.forEach(question => {
                // Click handler
                question.addEventListener('click', function() {
                    this.style.transform = 'scale(0.95)';
                    this.style.background = 'rgba(255, 255, 255, 0.4)';

                    setTimeout(() => {
                        this.style.transform = '';
                        this.style.background = '';
                    }, 150);

                    setTimeout(() => {
                        window.open(chatbotButton.href, '_blank');
                    }, 200);
                });

                // Keyboard handler for accessibility
                question.addEventListener('keydown', function(e) {
                    if (e.key === 'Enter' || e.key === ' ') {
                        e.preventDefault();
                        this.click();
                    }
                });
            });

            // Chatbot icon animation
            const chatbotIcon = document.querySelector('.chatbot-icon');
            if (chatbotIcon) {
                setInterval(() => {
                    chatbotIcon.style.filter = 'drop-shadow(0 4px 8px rgba(0, 0, 0, 0.3)) brightness(1.1)';
                    setTimeout(() => {
                        chatbotIcon.style.filter = 'drop-shadow(0 4px 8px rgba(0, 0, 0, 0.3))';
                    }, 500);
                }, 3000);
            }
        });
    </script>
</body>

</html>
